{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mode is set to false to avoid unwated testing outputs from the cells\n",
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y1.jpg', 'Y10.jpg', 'Y100.JPG', 'Y101.jpg', 'Y102.jpg', 'Y103.jpg', 'Y104.jpg', 'Y105.jpg', 'Y106.jpg', 'Y107.jpg', 'Y108.jpg', 'Y109.JPG', 'Y11.jpg', 'Y111.JPG', 'Y112.JPG', 'Y113.JPG', 'Y114.JPG', 'Y115.JPG', 'Y116.JPG', 'Y117.JPG', 'Y12.jpg', 'Y120.JPG', 'Y13.jpg', 'Y14.jpg', 'Y146.JPG', 'Y147.JPG', 'Y148.JPG', 'Y15.jpg', 'Y153.jpg', 'Y154.jpg', 'Y155.JPG', 'Y156.JPG', 'Y157.JPG', 'Y158.JPG', 'Y159.JPG', 'Y16.JPG', 'Y160.JPG', 'Y161.JPG', 'Y162.jpg', 'Y163.JPG', 'Y164.JPG', 'Y165.JPG', 'Y166.JPG', 'Y167.JPG', 'Y168.jpg', 'Y169.jpg', 'Y17.jpg', 'Y170.JPG', 'Y18.JPG', 'Y180.jpg', 'Y181.jpg', 'Y182.JPG', 'Y183.jpg', 'Y184.JPG', 'Y185.jpg', 'Y186.jpg', 'Y187.jpg', 'Y188.jpg', 'Y19.JPG', 'Y192.JPG', 'Y193.JPG', 'Y194.jpg', 'Y195.JPG', 'Y2.jpg', 'Y20.jpg', 'Y21.jpg', 'Y22.jpg', 'Y23.JPG', 'Y24.jpg', 'Y242.JPG', 'Y243.JPG', 'Y244.JPG', 'Y245.jpg', 'Y246.JPG', 'Y247.JPG', 'Y248.JPG', 'Y249.JPG', 'Y25.jpg', 'Y250.jpg', 'Y251.JPG', 'Y252.jpg', 'Y253.JPG', 'Y254.jpg', 'Y255.JPG', 'Y256.JPG', 'Y257.jpg', 'Y258.JPG', 'Y259.JPG', 'Y26.jpg', 'Y27.jpg', 'Y28.jpg', 'Y29.jpg', 'Y3.jpg', 'Y30.jpg', 'Y31.jpg', 'Y32.jpg', 'Y33.jpg', 'Y34.jpg', 'Y35.jpg', 'Y36.JPG', 'Y37.jpg', 'Y38.jpg', 'Y39.jpg', 'Y4.jpg', 'Y40.JPG', 'Y41.jpg', 'Y42.jpg', 'Y44.JPG', 'Y45.JPG', 'Y46.jpg', 'Y47.JPG', 'Y49.JPG', 'Y50.JPG', 'Y51.jpg', 'Y52.jpg', 'Y53.jpg', 'Y54.jpg', 'Y55.jpg', 'Y56.jpg', 'Y58.JPG', 'Y59.JPG', 'Y6.jpg', 'Y60.jpg', 'Y61.jpg', 'Y62.jpg', 'Y65.JPG', 'Y66.JPG', 'Y67.JPG', 'Y69.jpg', 'Y7.jpg', 'Y70.jpg', 'Y71.JPG', 'Y73.jpg', 'Y74.jpg', 'Y75.JPG', 'Y76.jpg', 'Y77.jpg', 'Y78.jpg', 'Y79.jpg', 'Y8.jpg', 'Y81.jpg', 'Y82.jpg', 'Y85.JPG', 'Y86.JPG', 'Y89.JPG', 'Y9.jpg', 'Y90.jpg', 'Y91.jpg', 'Y92.jpg', 'Y92.png', 'Y95.jpg', 'Y96.jpg', 'Y97.JPG', 'Y98.JPG', 'Y99.JPG']\n"
     ]
    }
   ],
   "source": [
    "YES_DIR_PATH = 'C:/Users/AbdullahAll.Mamun/project/Brain-Tumor-Detection/data/brain_tumor_dataset/yes/'\n",
    "NO_DIR_PATH ='C:/Users/AbdullahAll.Mamun/project/Brain-Tumor-Detection/data/brain_tumor_dataset/no/'\n",
    "# YES_DIR_PATH = 'brain_tumor_dataset/yes/'\n",
    "# NO_DIR_PATH = 'brain_tumor_dataset/no/'\n",
    "yes_imgs_name = os.listdir(YES_DIR_PATH)\n",
    "no_imgs_name = os.listdir(NO_DIR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    # Resize the image to 256x256 pixels\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        dsize=(256, 256),\n",
    "        interpolation=cv2.INTER_CUBIC\n",
    "    )\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(resized_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Apply a Gaussian blur to the image\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Threshold the image by Binary Thresholding\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # perform a series of erosions & dilations to remove any small regions of noise\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # find contours in thresholded image, then grab the largest one\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "    # find the extreme points\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "    # crop\n",
    "    ADD_PIXELS = 0\n",
    "    cropped_img = resized_img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS,\n",
    "                              extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use crop_image function to crop the images\n",
    "yes_imgs_cropped = [crop_image(cv2.imread(YES_DIR_PATH + img_file)) for img_file in yes_imgs_name]\n",
    "no_imgs_cropped = [crop_image(cv2.imread(NO_DIR_PATH + img_file)) for img_file in no_imgs_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_imgs = yes_imgs_cropped + no_imgs_cropped\n",
    "resized_imgs = [cv2.resize(img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC) for img in orig_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.squeeze(resized_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (test):\n",
    "    print(type(X))\n",
    "    print(X.shape)\n",
    "    print(X)\n",
    "    print(resized_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "X = X.astype('float32')\n",
    "X /= 255\n",
    "\n",
    "if (test):\n",
    "    print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_yes = np.full(len(yes_imgs_name), 1)\n",
    "labels_no = np.full(len(no_imgs_name), 0)\n",
    "\n",
    "img_labels = np.concatenate([labels_yes, labels_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (test):\n",
    "    print(img_labels.size, img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set (i.e `x_train`) and Testing set/Validation set (i.e `x_valid`)\n",
    "# We will also keep the original images of the validation set in `x_orig_valid` for visualization purposes\n",
    "yes_imgs      = X[:155]\n",
    "no_imgs       = X[155:]\n",
    "yes_orig_imgs = orig_imgs[:155]\n",
    "no_orig_imgs  = orig_imgs[155:]\n",
    "\n",
    "x_yes_train   = yes_imgs[:124]\n",
    "x_yes_valid   = yes_imgs[124:]\n",
    "x_yes_orig_valid = yes_orig_imgs[124:]\n",
    "\n",
    "x_no_train = no_imgs[:78]\n",
    "x_no_valid = no_imgs[78:]\n",
    "x_no_orig_valid = no_orig_imgs[78:]\n",
    "\n",
    "x_train = np.concatenate([x_yes_train, x_no_train])\n",
    "x_valid = np.concatenate([x_yes_valid, x_no_valid])\n",
    "#x_orig_valid = np.concatenate([x_yes_orig_valid, x_no_orig_valid])\n",
    "\n",
    "# Splitting the dataset labels for the Training set (i.e `y_train`) and Testing set/Validation set (i.e `y_valid`)\n",
    "yes_labels = img_labels[:155]\n",
    "no_labels = img_labels[155:]\n",
    "\n",
    "y_yes_train = yes_labels[:124]\n",
    "y_yes_valid = yes_labels[124:]\n",
    "\n",
    "y_no_train = no_labels[:78]\n",
    "y_no_valid = no_labels[78:]\n",
    "\n",
    "y_train = np.concatenate([y_yes_train, y_no_train])\n",
    "y_valid = np.concatenate([y_yes_valid, y_no_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=9,\n",
    "          padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.45))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=16, kernel_size=9, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=36, kernel_size=9, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.15))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model summary\n",
    "# model.summary() # Uncomment to see the model summary. Model summary is already been shown the cell below in SVG representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must install pydot (`pip install pydot`) for model_to_dot to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m SVG\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvis_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m model_to_dot\n\u001b[1;32m----> 3\u001b[0m SVG(model_to_dot(model, show_shapes\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mcreate(prog\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\AbdullahAll.Mamun\\project\\Brain-Tumor-Detection\\brain_tumor_detection\\lib\\site-packages\\keras\\utils\\vis_utils.py:138\u001b[0m, in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, subgraph, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Wrapper\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_pydot():\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou must install pydot (`pip install pydot`) for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel_to_dot to work.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m subgraph:\n\u001b[0;32m    144\u001b[0m     dot \u001b[39m=\u001b[39m pydot\u001b[39m.\u001b[39mCluster(style\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdashed\u001b[39m\u001b[39m\"\u001b[39m, graph_name\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mname)\n",
      "\u001b[1;31mImportError\u001b[0m: You must install pydot (`pip install pydot`) for model_to_dot to work."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('brain_tumor_detection': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06708e2d291adc24fdee19dc31964c2ca3cdb84e91de71eea448cf052f43e3dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
